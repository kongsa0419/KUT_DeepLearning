{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 과목명: 딥러닝및실습 hw1_2\n",
    "##### 실습자: 2017136063 여승준"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "324f896cc476501"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# a_2d_image_data.py\n",
    "\n",
    "NxCxHxW : 사진 수 / 사진의 채널수 / 높이 / 너비\n",
    "(* 3D이미지의 Depth만 빠짐)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b29fd4bcf4cac90"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(720, 1280, 3)\n",
      "uint8\n",
      "torch.uint8 torch.Size([3, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import torch\n",
    "\n",
    "'''\n",
    "아래에 imageio.imread()의 인자로 os.path.join()함수를 사용하는데, OS마다 파일지정 형식이 다를 수 있기 때문에 아래와 같은 방법이 절대주소/상대주소나 '..'을 지정해주는 것보다 안정적이다.\n",
    "'''\n",
    "\n",
    "\n",
    "img_arr = imageio.imread(os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\", \"bobby.jpg\"))\n",
    "print(type(img_arr)) # <class 'numpy.ndarray'>\n",
    "print(img_arr.shape) # (720, 1280, 3)\n",
    "print(img_arr.dtype) # uint8\n",
    "\n",
    "img = torch.from_numpy(img_arr)  # numpy.ndarray -> tensor\n",
    "out = img.permute(2, 0, 1) # 행과 열의 차원을 바꿔주는 함수 (직접 지정)\n",
    "print(out.dtype, out.shape) # torch.Size([3, 720, 1280])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T13:12:20.331955400Z",
     "start_time": "2023-09-19T13:12:19.101670200Z"
    }
   },
   "id": "fc29090c7399d81d"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 1\n",
      "['cat1.png', 'cat2.png', 'cat3.png']\n",
      "(256, 256, 3)\n",
      "uint8\n",
      "(256, 256, 3)\n",
      "uint8\n",
      "(256, 256, 3)\n",
      "uint8\n",
      "torch.Size([3, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50, 1)\n",
    "\n",
    "''' os.path.splittext()는 파일이름과 파일확장자명을 튜플로 반환 '''\n",
    "data_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "filenames = [\n",
    "  name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png'\n",
    "] \n",
    "print(filenames)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "  image = Image.open(os.path.join(data_dir, filename))\n",
    "  image.show()\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "  print(img_arr.shape)\n",
    "  print(img_arr.dtype)\n",
    "\n",
    "batch_size = 3\n",
    "'''(배치, 채널, H, W) 순서임을 유의'''\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8) \n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
    "  img_t = torch.from_numpy(img_arr)\n",
    "  img_t = img_t.permute(2, 0, 1) # (채널, H, W)\n",
    "  img_t = img_t[:3] # 알파채널 제거 코드\n",
    "  batch[i] = img_t # 각 배치마다 하나씩 이미지 가리키게 함\n",
    "\n",
    "print(batch.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T13:12:29.205833100Z",
     "start_time": "2023-09-19T13:12:19.132677800Z"
    }
   },
   "id": "f115fa20879586f9"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 2\n",
      "torch.float32\n",
      "torch.Size([3, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50, 2)\n",
    "\n",
    "batch = batch.float()\n",
    "batch /= 255.0\n",
    "print(batch.dtype)\n",
    "print(batch.shape)\n",
    "\n",
    "n_channels = batch.shape[1] #채널의 개수\n",
    "\n",
    "'''R,G,B 채널마다 돌면서 각각의 평균값, 표준편차 등을 구하여 저장'''\n",
    "for c in range(n_channels):\n",
    "  mean = torch.mean(batch[:, c])\n",
    "  std = torch.std(batch[:, c])\n",
    "  batch[:, c] = (batch[:, c] - mean) / std # 정규화"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T13:12:29.206830700Z",
     "start_time": "2023-09-19T13:12:29.193827900Z"
    }
   },
   "id": "c13fb82a3112fdaf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------\n",
    "# b_3d_image_data.py\n",
    "\n",
    "NxCxDxHxW : 사진 수 / 사진의 채널수 / 깊이 / 높이 / 너비"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6da379999317fe3f"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 99/99  (100.0%)\n",
      "<class 'imageio.core.util.Array'>\n",
      "(99, 512, 512)\n",
      "int16\n",
      "[[ -985  -990  -999 ... -1017 -1008  -971]\n",
      " [-1016  -984  -963 ... -1000 -1009  -999]\n",
      " [-1024 -1008  -996 ...  -979 -1021  -987]\n",
      " ...\n",
      " [ -920  -942  -944 ...  -893  -917  -955]\n",
      " [ -871  -879  -905 ...  -895  -869  -867]\n",
      " [ -876  -855  -873 ...  -933  -982  -936]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# 여러 3D 사진들이 저장된 폴더\n",
    "# .dcm = 의료 영상을 배포 및보기를위한 표준\n",
    "dir_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"c_volumetric-dicom\", \"2-LUNG_3.0_B70f-04083\")\n",
    "\n",
    "'''\n",
    "imageio.volread() 함수는 imageio 라이브러리를 사용하여 디렉토리에 있는 DICOM 형식의 의료 이미지 파일들을 읽어들이고, 이를 3D 볼륨(volume) 형태로 반환하는 함수입니다. 이 함수는 주로 의료 이미징 데이터를 처리하고 분석하는 데 사용됩니다.\n",
    "\n",
    "그럼 99개의 .dcm 파일은 한개의 3D 이미지로 봐야하나, 아니면 Depth가 99인 2D 이미지배열로 봐야하나? ==>같은 말인듯 \n",
    "'''\n",
    "vol_array = imageio.volread(dir_path, format='DICOM')\n",
    "print(type(vol_array))   # >>> <class 'imageio.core.util.Array'>:  Numpy NDArray\n",
    "print(vol_array.shape)   # >>> (99, 512, 512) (N x C) x D x H x W\n",
    "print(vol_array.dtype)   # >>> int16\n",
    "print(vol_array[0]) # 1번째 사진"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T13:12:29.735903900Z",
     "start_time": "2023-09-19T13:12:29.209831400Z"
    }
   },
   "id": "fddb34f271d73b79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 1\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) # 사이즈가 10,10인 그림을 생성\n",
    "for id in range(0, 99): #매 사진마다\n",
    "  fig.add_subplot(10, 10, id + 1) # id+1번째 인덱스에 그림 삽입\n",
    "  plt.imshow(vol_array[id])\n",
    "plt.show()\n",
    "\n",
    "import torch\n",
    "\n",
    "vol = torch.from_numpy(vol_array).float()\n",
    "vol = torch.unsqueeze(vol, 0)  # channel\n",
    "vol = torch.unsqueeze(vol, 0)  # data size\n",
    "\n",
    "print(vol.shape)  # >>> torch.Size([1, 1, 99, 512, 512])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-19T13:12:29.731902Z"
    }
   },
   "id": "15e200620e1342b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 2)\n",
    "\n",
    "'''\n",
    "torch.mean, sum, std 는 dimension이 축소됌\n",
    "\n",
    "keepdim=True: 이 옵션을 사용하면 결과 텐서의 차원 수가 입력 텐서와 동일한 상태로 유지됩니다. 즉, 결과 텐서는 입력 텐서와 동일한 차원 수를 가지며, 차원 크기 중 하나는 1이 될 수 있습니다. 이 옵션을 사용하면 브로드캐스팅 연산에 도움이 됩니다.\n",
    "\n",
    "keepdim=False (기본값): 이 옵션을 사용하면 결과 텐서는 입력 텐서의 차원 중 크기가 1인 차원이 제거되고, 해당 차원이 없는 텐서로 축소됩니다. 결과 텐서의 차원 수는 입력 텐서보다 1 차원 줄어들 수 있습니다.\n",
    "'''\n",
    "mean = torch.mean(vol, dim=(3, 4), keepdim=True)\n",
    "print(mean.shape) # torch.Size([1, 1, 99, 1, 1])\n",
    "std = torch.std(vol, dim=(3, 4), keepdim=True)\n",
    "print(std.shape) # torch.Size([1, 1, 99, 1, 1])\n",
    "vol = (vol - mean) / std\n",
    "print(vol.shape)\n",
    "\n",
    "print(vol[0, 0, 0])\n",
    "\n",
    "keepdim_test = torch.mean(vol, dim=(3,4), keepdim=False)\n",
    "print(keepdim_test.shape) # torch.Size([1,1,99])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9d87305f2ae2b726"
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------\n",
    "# c_tabular_wine_data.py\n",
    "한 행당 샘플을 나타내는 테이블(표) 데이터.\n",
    "한 열마다 특징값을 나타냄.\n",
    "쉽게 생각해서 엑셀 표를 생각하면 된다. \n",
    "<br><br>\n",
    "#### NxF : 데이터크기 x 특징값(피쳐) 수\n",
    "보통 마지막 column에 target(예측값)을 설정함.\n",
    "마지막 column을 제외한 나머지 열에 대한 data들을 학습시켜 가장 유의미한 target을 예측해야 함\n",
    "<br><br>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17906e50fa40c527"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "'''\n",
    "csv : Comma Seperated Values\n",
    "주로 ',' , ';' 등으로 구분 됌\n",
    "delimiter = \";\" ==> ';'으로 데이터 구분\n",
    "skiprows=1 ==> 첫 카테고리 문자열 행은 데이터가 아님 (skip) \n",
    "'''\n",
    "\n",
    "wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "print(wineq_numpy.dtype)\n",
    "print(wineq_numpy.shape) # (4898, 12)\n",
    "print(wineq_numpy)\n",
    "print()\n",
    "\n",
    "col_list = next(csv.reader(open(wine_path), delimiter=';')) # iterator 반환 \n",
    "print(col_list)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "53fc7ae864f148d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 1)\n",
    "\n",
    "import torch\n",
    "\n",
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "print(wineq.dtype)\n",
    "print(wineq.shape) #torch.Size([4898, 12])\n",
    "print()\n",
    "\n",
    "data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "target = wineq[:, -1]  # Selects all rows and the last column\n",
    "print(target.dtype)\n",
    "print(target.shape) # torch.Size([4898])\n",
    "print(target)\n",
    "print()\n",
    "\n",
    "target = target.long()  # treat labels as an integer\n",
    "print(target.dtype)\n",
    "print(target.shape)\n",
    "print(target)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c4cf3e9f0490480d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 2)\n",
    "\n",
    "'''왜 피쳐값이 11개가 아니고 10개지? 아마 id같은 피쳐는 활용 안하려고 그런가?'''\n",
    "eye_matrix = torch.eye(10)\n",
    "# We use the 'target' tensor as indices to extract the corresponding rows from the identity matrix\n",
    "# It can generate the one-hot vectors for each element in the 'target' tensor\n",
    "onehot_target = eye_matrix[target]\n",
    "\n",
    "print(onehot_target.shape)  # >>> torch.Size([4898, 10])\n",
    "print(onehot_target[0])\n",
    "print(onehot_target[1])\n",
    "print(onehot_target[-2])\n",
    "print(onehot_target)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "532aaf8fc65db754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 3)\n",
    "\n",
    "'''keepdim=False가 디폴트라서 mean, var 연산 후 dimension이 줄어듦을 확인할 수 있다.'''\n",
    "\n",
    "data_mean = torch.mean(data, dim=0) #(4898, 11)\n",
    "data_var = torch.var(data, dim=0) #분산\n",
    "data = (data - data_mean) / torch.sqrt(data_var) # 정규화\n",
    "print(data_mean.shape) #Size([11])\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9b163b8036b8e368"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 4)\n",
    "\n",
    "# scikit-learn이라는 유명 패키지\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "# data : Size([4898,11])\n",
    "# onehot_target : Size([4898,10])\n",
    "# test_size=0.2 : train(80%), test(20%) \n",
    "'''\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "'''\n",
    "y의 column값이 10인 이유: onehot-encoding\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a77242e01e94c3b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 그동안 배운 것 응용한 함수\n",
    "def get_wine_data():\n",
    "  wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "  wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "\n",
    "  wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "  data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "  target = wineq[:, -1].long()  # treat labels as an integer\n",
    "\n",
    "  eye_matrix = torch.eye(10)\n",
    "  onehot_target = eye_matrix[target]\n",
    "\n",
    "  data_mean = torch.mean(data, dim=0)\n",
    "  data_var = torch.var(data, dim=0)\n",
    "  data = (data - data_mean) / torch.sqrt(data_var) #정규화\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(data, onehot_target, test_size=0.2)\n",
    "\n",
    "  return X_train, X_valid, y_train, y_valid"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "31b824c8021c90ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# d_tabular_california_housing.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9139a80ef3c2b165"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/implement-linear-regression-on-boston-housing-dataset-by-pytorch-c5d29546f938\n",
    "# https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\n",
    "import torch\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "'''\n",
    "캘리포니아 부동산에 관한 유명한 데이터셋 분석\n",
    "'''\n",
    "housing = fetch_california_housing()\n",
    "print(housing.keys())\n",
    "\n",
    "print(type(housing.data))\n",
    "print(housing.data.dtype)\n",
    "print(housing.data.shape)\n",
    "print(housing.feature_names)\n",
    "\n",
    "print(housing.target.shape)\n",
    "print(housing.target_names)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ee668d8aafc11f0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(housing.data.min(), housing.data.max())\n",
    "\n",
    "# axis=0 : 각 열마다 (feature마다) 계산\n",
    "data_mean = np.mean(housing.data, axis=0)\n",
    "data_var = np.var(housing.data, axis=0)\n",
    "data = (housing.data - data_mean) / np.sqrt(data_var) # 정규화\n",
    "target = housing.target\n",
    "\n",
    "print(housing.data.shape)\n",
    "print(data_mean.shape)\n",
    "print(data.min(), data.max())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a638eb0d10e8418a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_valid = torch.from_numpy(X_valid)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_valid = torch.from_numpy(y_valid)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6c9a40eb3c946ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# e_time_series_data.py\n",
    "\n",
    "NxLxF\n",
    " :N(Data size), L(Length==hours), F(Features)\n",
    " \n",
    "이벤트 기준 : 정량적 시간 간격 x\n",
    "시간 기준 : 정량적 시간 간격"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae73800b090f729d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)\n",
    "\n",
    "bikes_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "bikes_numpy = np.loadtxt(\n",
    "  fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "  converters={\n",
    "    1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7.0\n",
    "  }\n",
    ")\n",
    "'''converters 문 : 일자별로 자름'''\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "print(bikes.shape)\n",
    "print(bikes)\n",
    "\n",
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "print(daily_bikes.shape)  # >>> torch.Size([730, 24, 17]) \n",
    "# N(size) x L(hrs) x F(features)\n",
    "\n",
    "daily_bikes_data = daily_bikes[:, :, :-1]\n",
    "daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1) #차원을 동일하게 맞춰줌\n",
    "\n",
    "print(daily_bikes_data.shape)\n",
    "print(daily_bikes_target.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4ecf692f5893f4fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 1)\n",
    "\n",
    "first_day_data = daily_bikes_data[0]\n",
    "print(first_day_data.shape) # torch.Size([24, 13])\n",
    "\n",
    "'''정량데이터 - 카테고리 - NomialData (not ordinal)'''\n",
    "# Whether situation: 1: clear, 2:mist, 3: light rain/snow, 4: heavy rain/snow\n",
    "print(first_day_data[:, 9].long()) #9th feature인 weather situation을 가져온다. (24h 모든 데이터를) ==> Size(24,1)\n",
    "eye_matrix = torch.eye(4) # 단위행렬\n",
    "print(eye_matrix)\n",
    "\n",
    "weather_onehot = eye_matrix[first_day_data[:, 9].long() - 1]\n",
    "print(weather_onehot.shape) #Size(24,4)\n",
    "print(weather_onehot)\n",
    "\n",
    "first_day_data_torch = torch.cat(tensors=(first_day_data, weather_onehot), dim=1)\n",
    "# cat([Size(24,16), Size(24,4)], dim=1)\n",
    "# ==> Size(24,20)\n",
    "print(first_day_data_torch.shape) # ==> Size(24,17)\n",
    "print(first_day_data_torch)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6657668c0c1c927d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 2)\n",
    "\n",
    "day_data_torch_list = []\n",
    "for daily_idx in range(daily_bikes_data.shape[0]):  # range(730)\n",
    "  day = daily_bikes_data[daily_idx]  # day.shape: [24, 13]\n",
    "  weather_onehot = eye_matrix[day[:, 9].long() - 1] #[(24,4)]\n",
    "  # print(day.shape, weather_onehot.shape, sep=\"\\n\")\n",
    "  day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # day_torch.shape: [24, 20]\n",
    "  day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "print(len(day_data_torch_list))\n",
    "daily_bikes_data = torch.stack(day_data_torch_list, dim=0)\n",
    "print(daily_bikes_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "19347b916bd6143b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 3)\n",
    "\n",
    "print(daily_bikes_data[:, :, :9].shape, daily_bikes_data[:, :, 10:].shape)\n",
    "daily_bikes_data = torch.cat(\n",
    "  [daily_bikes_data[:, :, :9], daily_bikes_data[:, :, 10:]],\n",
    "  dim=2\n",
    ")\n",
    "print(daily_bikes_data.shape)\n",
    "\n",
    "temperatures = daily_bikes_data[:, :, 9] # >>> torch.Size([730, 17])\n",
    "daily_bikes_data[:, :, 9] = (daily_bikes_data[:, :, 9] - torch.mean(temperatures)) / torch.std(temperatures)\n",
    "\n",
    "# daily_bikes_data = daily_bikes_data.transpose(1, 2)\n",
    "print(daily_bikes_data.shape)  # >>> torch.Size([730, 17, 24])\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8c5be3425043048"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# f_audio_data.py\n",
    "\n",
    "NxCxL : (Size) x (Channel) x (Length)\n",
    " \n",
    "spectrogram : 오디오 신호나 소리의 주파수와 시간에 대한 시각적 표현\n",
    "NxCxFxT : (Size) x (Channel) x (Freq) x (Time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de30e548089daa1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy.io.wavfile as wavfile\n",
    "\n",
    "audio_1_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100038-A-14.wav\")\n",
    "audio_2_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"f_audio-chirp\", \"1-100210-A-36.wav\")\n",
    "\n",
    "freq_1, waveform_arr_1 = wavfile.read(audio_1_path)\n",
    "print(freq_1) # 44100\n",
    "print(type(waveform_arr_1))\n",
    "print(len(waveform_arr_1)) # 220_500 <== 44100 * 5s\n",
    "print(waveform_arr_1)\n",
    "\n",
    "freq_2, waveform_arr_2 = wavfile.read(audio_2_path)\n",
    "\n",
    "# \n",
    "# '_'언더스코어로 가독성 좋은 수 표현\n",
    "waveform = torch.empty(2, 1, 220_500)\n",
    "\n",
    "waveform[0, 0] = torch.from_numpy(waveform_arr_1).float()\n",
    "waveform[1, 0] = torch.from_numpy(waveform_arr_2).float()\n",
    "print(waveform.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5f4beb5da7d519f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"#\" * 50, 1)\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "_, _, sp_arr_1 = signal.spectrogram(waveform_arr_1, freq_1)\n",
    "_, _, sp_arr_2 = signal.spectrogram(waveform_arr_2, freq_2)\n",
    "\n",
    "sp_1 = torch.from_numpy(sp_arr_1)\n",
    "sp_2 = torch.from_numpy(sp_arr_2)\n",
    "print(sp_1.shape) # torch.Size([129, 984]) <== Frequency & time\n",
    "print(sp_2.shape) # torch.Size([129, 984]) <== Frequency & time\n",
    "\n",
    "sp_left_t = torch.from_numpy(sp_arr_1)\n",
    "sp_right_t = torch.from_numpy(sp_arr_2)\n",
    "print(sp_left_t.shape) # torch.Size([129, 984])\n",
    "print(sp_right_t.shape) # torch.Size([129, 984])\n",
    "\n",
    "sp_t = torch.stack((sp_left_t, sp_right_t), dim=0).unsqueeze(dim=0)\n",
    "print(sp_t.shape) # torch.Size([1, 2, 129, 984])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "664b2904018e9159"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# g_video_data.py\n",
    "\n",
    "NxCxTxHxW : (Size) x (Channel) x (Time=frame) x (Height) x (Width)\n",
    " \n",
    "* pip install imageio[ffmpeg] 설치 필요\n",
    "* ffmpeg : A complete, cross-platform solution to record, convert and stream audio and video"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "152f4f25f82efffc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pip install imageio[ffmpeg]\n",
    "import torch\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "video_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"g_video-cockatoo\", \"cockatoo.mp4\")\n",
    "\n",
    "reader = imageio.get_reader(video_path)\n",
    "print(type(reader)) # <class 'imageio.plugins.ffmpeg.FfmpegFormat.Reader'>\n",
    "meta = reader.get_meta_data()\n",
    "print(meta)\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "  frame = torch.from_numpy(frame).float()  # frame.shape: [360, 480, 3]\n",
    "  # print(i, frame.shape)   # i, torch.Size([360, 480, 3])\n",
    "\n",
    "n_channels = 3\n",
    "n_frames = 529\n",
    "video = torch.empty(1, n_frames, n_channels, *meta['size'])  # (1, 529, 3, 480, 360)\n",
    "print(video.shape)\n",
    "\n",
    "\n",
    "for i, frame in enumerate(reader):\n",
    "  frame = torch.from_numpy(frame).float()       # frame.shape: [360, 480, 3]\n",
    "  frame = torch.permute(frame, dims=(2, 1, 0))  # frame.shape: [3, 480, 360]\n",
    "  video[0, i] = frame\n",
    "\n",
    "video = video.permute(dims=(0, 2, 1, 3, 4)) # [1, 3, 529, 480, 360]\n",
    "print(video.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d1bfc00137486687"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------------------------------------------\n",
    "# h_linear_regression_dataset_dataloader.py\n",
    "\n",
    "##### torch.utils.data.Dataset\n",
    "- stores data samples and expected target values (labels)\n",
    "- returns one sample at a time \n",
    "##### torch.utils.data.Dataloader\n",
    "- groups data in batches, iterate them, and enable multi-processing\n",
    "\n",
    "\n",
    "[usage]\n",
    "- Dataset 재정의 시 __init__, __len__, __getitem__ 을 재정의 해줘야 함 (필수적 권장)\n",
    "- DataLoader 의 매개변수\n",
    "  - batch_size : 배치 크기 지정\n",
    "  - shuffle : 매 epoch마다 overfitting을 줄이기 위해서 데이터를 섞을지 말지 결정\n",
    "    - training 데이터셋에 대해서는 true\n",
    "    - testing 데이터셋에 대해서는 false\n",
    "  - num_workers : multi-processor 수 (default: 0)\n",
    "  - drop_last : 배치 사이즈에 따라서 마지막 배치의 record 수가 적을수도 있고 클 수도 있다. 그에 따라서 이것이 훈련에 도움이 될수도 있고, 오히려 악영향을 끼칠 수도 있다. 이를 결정하는 것이다. (소수의 잘못된 데이터를 학습하면 학습효과가 떨어진다. 반면에 버려지는 데이터가 아까우니 조금의 훈련이라도 더 하면 좋지 않느냐는 생각도 있다.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2619259a3c2376e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "'''커스텀 Dataset 정의'''\n",
    "class LinearRegressionDataset(Dataset):\n",
    "  def __init__(self, N=50, m=-3, b=2, *args, **kwargs):\n",
    "    # N: number of samples, e.g. 50\n",
    "    # m: slope\n",
    "    # b: offset\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "    self.x = torch.rand(N, 2) # 균등 분포\n",
    "    self.noise = torch.rand(N) * 0.2\n",
    "    self.m = m\n",
    "    self.b = b\n",
    "    self.y = (torch.sum(self.x * self.m) + self.b + self.noise).unsqueeze(-1)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {'input': self.x[idx], 'target': self.y[idx]}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.x), self.x.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  linear_regression_dataset = LinearRegressionDataset()\n",
    "\n",
    "  print(linear_regression_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(linear_regression_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'], sample['target']))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(linear_regression_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  ''' random_split() 으로 7:2:1 비율로 데이터가 잘 나뉘었는지 확인 '''\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True # train_dataset에는 shuffle ON\n",
    "  )\n",
    "\n",
    "  '''DataLoader 출력 결과, 배치사이즈 4임을 확인'''\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'], batch['target']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "599c4e590b60042d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# i_2d_image_dataset_dataloader.py\n",
    "\n",
    "Dog&Cat 2D Image Dataset Example"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d59c432ecd2bcf35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DogCat2DImageDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.image_transforms = transforms.Compose([\n",
    "      transforms.Resize(size=(256, 256)),\n",
    "      transforms.ToTensor()\n",
    "    ]) # 이미지 변환에 필요한 함수 목록 정의\n",
    "\n",
    "    dogs_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\")\n",
    "    cats_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "\n",
    "\n",
    "    # bobby.jpg 같은 경우, resize() 필요\n",
    "    # 모든 image들을 numpy -> tensor로 만들어줘야\n",
    "    image_lst = [\n",
    "      Image.open(os.path.join(dogs_dir, \"bobby.jpg\")),  # (1280, 720, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat1.png\")),  # (256, 256, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat2.png\")),  # (256, 256, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat3.png\"))  # (256, 256, 3)\n",
    "    ]\n",
    "\n",
    "    image_lst = [self.image_transforms(img) for img in image_lst]\n",
    "    \n",
    "    self.images = torch.stack(image_lst, dim=0) #[4, 3, 256, 256]\n",
    "    \n",
    "\n",
    "    # 0: \"dog\", 1: \"cat\"\n",
    "    self.image_labels = torch.tensor([[0], [1], [1], [1]])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {'input': self.images[idx], 'target': self.image_labels[idx]}\n",
    "\n",
    "  # print()시 객체정보 출력\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.images), self.images.shape, self.image_labels.shape\n",
    "    )\n",
    "    return str"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9b64a3e706a7e2ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "  dog_cat_2d_image_dataset = DogCat2DImageDataset()\n",
    "\n",
    "  print(dog_cat_2d_image_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(dog_cat_2d_image_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'].shape, sample['target']))\n",
    "\n",
    "  train_dataset, test_dataset = random_split(dog_cat_2d_image_dataset, [0.7, 0.3])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "68041d22ed213b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------\n",
    "# j_wine_dataset_dataloader.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8ec27befcc241a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    wine_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"d_tabular-wine\", \"winequality-white.csv\")\n",
    "    wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "    wineq = torch.from_numpy(wineq_numpy)\n",
    "\n",
    "    data = wineq[:, :-1]  # Selects all rows and all columns except the last\n",
    "    data_mean = torch.mean(data, dim=0)\n",
    "    data_var = torch.var(data, dim=0)\n",
    "    self.data = (data - data_mean) / torch.sqrt(data_var)\n",
    "\n",
    "    target = wineq[:, -1].long()  # treat labels as an integer\n",
    "    eye_matrix = torch.eye(10)\n",
    "    self.target = eye_matrix[target]\n",
    "\n",
    "    assert len(self.data) == len(self.target)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    wine_feature = self.data[idx]\n",
    "    wine_target = self.target[idx]\n",
    "    return {'input': wine_feature, 'target': wine_target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.data), self.data.shape, self.target.shape\n",
    "    )\n",
    "    return str"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c5ccf391332b8af6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "  wine_dataset = WineDataset()\n",
    "\n",
    "  print(wine_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(wine_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'].shape, sample['target'].shape))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(wine_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9740dbbf2e683f8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------------\n",
    "# k_california_housing_dataset_dataloader.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32be29a16664d484"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class CaliforniaHousingDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    housing = fetch_california_housing()\n",
    "    data_mean = np.mean(housing.data, axis=0)\n",
    "    data_var = np.var(housing.data, axis=0)\n",
    "    self.data = torch.tensor((housing.data - data_mean) / np.sqrt(data_var), dtype=torch.float32)\n",
    "    self.target = torch.tensor(housing.target, dtype=torch.float32).unsqueeze(dim=-1)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample_data = self.data[idx]\n",
    "    sample_target = self.target[idx]\n",
    "    return {'input': sample_data, 'target': sample_target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.data), self.data.shape, self.target.shape\n",
    "    )\n",
    "    return str\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "64514b1894ea2bab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  california_housing_dataset = CaliforniaHousingDataset()\n",
    "\n",
    "  print(california_housing_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(california_housing_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'].shape, sample['target'].shape))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(california_housing_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2b22fb2af2efbc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------------------------------------\n",
    "# l_time_series_dataset_dataloader.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cdfaaa9d9840de1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class BikesDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    bikes_path = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "    bikes_numpy = np.loadtxt(\n",
    "      fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "      converters={\n",
    "        1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "      }\n",
    "    )\n",
    "    bikes = torch.from_numpy(bikes_numpy)\n",
    "\n",
    "    daily_bikes = bikes.view(-1, 24, bikes.shape[1])  # daily_bikes.shape: torch.Size([730, 24, 17])\n",
    "    self.daily_bikes_target = daily_bikes[:, :, -1].unsqueeze(dim=-1)\n",
    "\n",
    "    self.daily_bikes_data = daily_bikes[:, :, :-1]\n",
    "    eye_matrix = torch.eye(4)\n",
    "\n",
    "    day_data_torch_list = []\n",
    "    for daily_idx in range(self.daily_bikes_data.shape[0]):  # range(730)\n",
    "      day = self.daily_bikes_data[daily_idx]  # day.shape: [24, 17]\n",
    "      weather_onehot = eye_matrix[day[:, 9].long() - 1]\n",
    "      day_data_torch = torch.cat(tensors=(day, weather_onehot), dim=1)  # day_torch.shape: [24, 21]\n",
    "      day_data_torch_list.append(day_data_torch)\n",
    "\n",
    "    self.daily_bikes_data = torch.stack(day_data_torch_list, dim=0)\n",
    "\n",
    "    self.daily_bikes_data = torch.cat(\n",
    "      [self.daily_bikes_data[:, :, :9], self.daily_bikes_data[:, :, 10:]], dim=2\n",
    "    )\n",
    "\n",
    "    temperatures = self.daily_bikes_data[:, :, 9]\n",
    "    self.daily_bikes_data[:, :, 9] = \\\n",
    "      (self.daily_bikes_data[:, :, 9] - torch.mean(temperatures)) / torch.std(temperatures)\n",
    "\n",
    "    assert len(self.daily_bikes_data) == len(self.daily_bikes_target)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.daily_bikes_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    bike_feature = self.daily_bikes_data[idx]\n",
    "    bike_target = self.daily_bikes_target[idx]\n",
    "    return {'input': bike_feature, 'target': bike_target}\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.daily_bikes_data), self.daily_bikes_data.shape, self.daily_bikes_target.shape\n",
    "    )\n",
    "    return str"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "dc0cee165ae2caa8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  bikes_dataset = BikesDataset()\n",
    "  print(bikes_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(bikes_dataset):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, sample['input'].shape, sample['target'].shape))\n",
    "\n",
    "  train_dataset, validation_dataset, test_dataset = random_split(bikes_dataset, [0.7, 0.2, 0.1])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f685fc8587b37630"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---------------------------------\n",
    "# 숙제후기\n",
    "\n",
    "- 교수님께서 기본기와 shape 찍어보는 걸 많이 강조하셨는데 차원이 많아질수록 복잡해지는 걸 보니 왜 그런 말씀을 하셨는지 알 수 있었다. (그리고 새로운걸 더 배울수록 점점 기초적인 것을 까먹을 것 같아서 기본기에 미리 많이 익숙해져야겠다)\n",
    "- 코드의 질이 참 훌륭하고, 교안이 좋아서 교안만 봐도 교수님께서 수업시간에 말씀하신 내용이 생각났다. 이런 점이 좋았다.\n",
    "- 이론만 배우고 \"자 됐죠? 알아서 코딩해오세요\" 하는 방식보다 이렇게 충분한 레퍼런스를 받고 어떻게 이를 응용해서 적용시킬 수 있는지를 과제로 내주셨으면 좋겠다. 코드를 보면 이해가 잘 간다. 처음부터 코드를 이렇게 짜라고 하면 무척 헤멜 것 같다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b4edb28bdce98e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
